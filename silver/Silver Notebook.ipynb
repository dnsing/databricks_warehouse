{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0f41d65-63b1-4886-aba0-044e378607fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Silver Source CRM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd78d7e9-47cf-407f-b010-12b5a5a8689c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from pyspark.sql.functions import col, count, when, row_number, desc, trim\n",
    "from pyspark.sql.functions import substring, regexp_replace, when, length, expr, lead, lit\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85048947-ac94-4111-99b6-8c163014e902",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_to_one_csv(df, destination_folder_path, destination_file_name):\n",
    "    # Make dateframe into a single partition and write to CSV\n",
    "    df.coalesce(1).write.mode(\"append\").csv(destination_folder_path, header=True) \n",
    "    # Remove unneeded files, and rename csv file to template\n",
    "    list_dir = dbutils.fs.ls(destination_folder_path)\n",
    "    for file in list_dir:\n",
    "        file_name = file.name\n",
    "        file_path = file.path\n",
    "        if file_name.startswith(\"part\"):\n",
    "            source_path = f\"{destination_folder_path}/{file_name}\"\n",
    "            destination_path = f\"{destination_folder_path}/{destination_file_name}\"\n",
    "            dbutils.fs.mv(source_path, destination_path)\n",
    "            print(f\"{file_name} renamed to {destination_file_name}\")\n",
    "        elif file_name.endswith(\".csv\"):\n",
    "            pass\n",
    "        else:\n",
    "            file_path = f\"{destination_folder_path}/{file_name}\"\n",
    "            dbutils.fs.rm(file_path)\n",
    "            print(f\"{file_name} removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddc530a3-475d-4805-bfa6-9e489106ada2",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1752262381045}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# cust_info\n",
    "silver_cust_info = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/Volumes/workspace/default/bronze/source_crm/cust_info.csv\")\n",
    "\n",
    "# Data Cleaning\n",
    "# Keeping latest value only\n",
    "window_spec = Window.partitionBy(\"cst_id\").orderBy(desc(\"cst_create_date\"))\n",
    "silver_cust_info = silver_cust_info.withColumn(\"rn\", row_number().over(window_spec)).filter(col(\"rn\") == 1).drop(\"rn\")\n",
    "\n",
    "# Trim string values and alias them back to original names\n",
    "silver_cust_info = silver_cust_info.select(\n",
    "    *[trim(col(c)).alias(c) if dtype == \"string\" else col(c) for c, dtype in silver_cust_info.dtypes]\n",
    ")\n",
    "\n",
    "# Renaming columns\n",
    "silver_cust_info = silver_cust_info.withColumn(\n",
    "    \"cst_marital_status\",\n",
    "    when(col(\"cst_marital_status\") == \"M\", \"Married\")\n",
    "   .when(col(\"cst_marital_status\") == \"S\", \"Single\")\n",
    "   .otherwise('na')\n",
    ")\n",
    "silver_cust_info = silver_cust_info.withColumn(\n",
    "    \"cst_gndr\",\n",
    "    when(col(\"cst_gndr\") == \"M\", \"Male\")\n",
    "   .when(col(\"cst_gndr\") == \"F\", \"Female\")\n",
    "   .otherwise('na')\n",
    ")\n",
    "\n",
    "# Drop nulls and duplicates\n",
    "silver_cust_info = silver_cust_info.dropDuplicates()\n",
    "silver_cust_info = silver_cust_info.dropna(how='any', subset=['cst_id'])\n",
    "\n",
    "write_to_one_csv(silver_cust_info, \"/Volumes/workspace/default/silver/source_crm\", \"cust_info.csv\")\n",
    "\n",
    "# display(\n",
    "#     silver_cust_info.groupBy(\"cst_id\").agg(count(\"*\").alias(\"count\")).filter((col(\"count\") > 1) | col(\"cst_id\").isNull())\n",
    "# )\n",
    "# display(silver_cust_info.select(\"cst_key\").distinct())\n",
    "# display(silver_cust_info.filter(col(\"cst_id\").isNull()))\n",
    "\n",
    "display(silver_cust_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de7b280e-ae1d-4409-9594-f4717400cf0a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1752268460911}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# prd_info\n",
    "silver_prd_info = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/Volumes/workspace/default/bronze/source_crm/prd_info.csv\")\n",
    "\n",
    "# Cleaning Data\n",
    "silver_prd_info = silver_prd_info.withColumn(\n",
    "    \"cat_id\",\n",
    "    regexp_replace(substring(\"prd_key\", 1, 5), \"-\", \"_\")\n",
    ")\n",
    "\n",
    "silver_prd_info = silver_prd_info.withColumn(\n",
    "    \"prd_key\",\n",
    "    substring(\"prd_key\", 7, length(\"prd_key\"))\n",
    ")\n",
    "\n",
    "# Trim string values and alias them back to original names\n",
    "silver_prd_info = silver_prd_info.select(\n",
    "    *[trim(col(c)).alias(c) if dtype == \"string\" else col(c) for c, dtype in silver_prd_info.dtypes]\n",
    ")\n",
    "\n",
    "# Handling null values\n",
    "silver_prd_info = silver_prd_info.withColumn(\n",
    "    \"prd_cost\",\n",
    "    when(col(\"prd_cost\").isNull(), 0).otherwise(col(\"prd_cost\"))\n",
    ")\n",
    "\n",
    "# Renaming columns\n",
    "silver_prd_info = silver_prd_info.withColumn(\n",
    "    \"prd_line\",\n",
    "    when(col(\"prd_line\") == \"M\", \"Mountain\")\n",
    "   .when(col(\"prd_line\") == \"R\", \"Road\")\n",
    "   .when(col(\"prd_line\") == \"S\", \"Other Sales\")\n",
    "   .when(col(\"prd_line\") == \"T\", \"Touring\")\n",
    "   .otherwise('na')\n",
    ")\n",
    "\n",
    "# End date enhancement\n",
    "# Pyspark solution\n",
    "w = Window.partitionBy(\"prd_key\").orderBy(\"prd_start_dt\")\n",
    "silver_prd_info = silver_prd_info.withColumn(\n",
    "    \"prd_end_dt\",\n",
    "        lead((\"prd_start_dt\")).over(w)-1  \n",
    "    )\n",
    "\n",
    "# SQL solution\n",
    "# silver_prd_info = silver_prd_info.withColumn(\n",
    "#     \"prd_end_dt\",\n",
    "#     expr(\"case \n",
    "#               when prd_end_dt is null then null \n",
    "#               else lead(prd_start_dt) over (partition by prd_key order by prd_start_dt)-1 \n",
    "#           end\")\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# Swap prd_start_dt and prd_end_dt if prd_start_dt > prd_end_dt\n",
    "# silver_prd_info = silver_prd_info.withColumn(\n",
    "#     \"temp_start_dt\",\n",
    "#     when(col(\"prd_start_dt\") > col(\"prd_end_dt\"), col(\"prd_end_dt\")).otherwise(col(\"prd_start_dt\"))\n",
    "# ).withColumn(\n",
    "#     \"temp_end_dt\",\n",
    "#     when(col(\"prd_start_dt\") > col(\"prd_end_dt\"), col(\"prd_start_dt\")).otherwise(col(\"prd_end_dt\"))\n",
    "# ).drop(\"prd_start_dt\", \"prd_end_dt\").withColumnRenamed(\"temp_start_dt\", \"prd_start_dt\").withColumnRenamed(\"temp_end_dt\", \"prd_end_dt\")\n",
    "\n",
    "# display(silver_prd_info.groupby(\"prd_key\").)\n",
    "\n",
    "# display(silver_prd_info.where(col(\"prd_start_dt\") > col(\"prd_end_dt\")))\n",
    "# display(silver_prd_info.select(\"prd_line\").distinct())\n",
    "# display(silver_prd_info.select(col(\"prd_cost\").isNull()))\n",
    "# display(silver_prd_info.filter(silver_prd_info[\"prd_cost\"].isNull()))\n",
    "# display(silver_prd_info.groupby(\"prd_id\").count().where((col('count') > 1) | col('prd_id').isNull()))\n",
    "\n",
    "write_to_one_csv(silver_prd_info, \"/Volumes/workspace/default/silver/source_crm\", \"prd_info.csv\")\n",
    "display(silver_prd_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b99acf94-1ecf-490c-b92d-6a36834ab902",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"sls_cust_id\":126},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1752362445648}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, abs\n",
    "\n",
    "# sales_details\n",
    "silver_sales_details = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/Volumes/workspace/default/bronze/source_crm/sales_details.csv\")\n",
    "\n",
    "# Data Cleaning\n",
    "# Handling date data\n",
    "silver_sales_details = silver_sales_details.withColumn(\n",
    "    \"sls_order_dt\",\n",
    "    to_date(when((col(\"sls_order_dt\") == 0) | (length(col(\"sls_order_dt\")) != 8), None).otherwise(col(\"sls_order_dt\")), \"yyyyMMdd\")\n",
    ").withColumn(\n",
    "    \"sls_ship_dt\",\n",
    "    to_date(when((col(\"sls_ship_dt\") == 0) | (length(col(\"sls_ship_dt\")) != 8), None).otherwise(col(\"sls_ship_dt\")), \"yyyyMMdd\")\n",
    ").withColumn(\n",
    "    \"sls_due_dt\",\n",
    "    to_date(when((col(\"sls_due_dt\") == 0) | (length(col(\"sls_due_dt\")) != 8), None).otherwise(col(\"sls_due_dt\")), \"yyyyMMdd\")\n",
    ")\n",
    "\n",
    "# Trim string values and alias them back to original names\n",
    "silver_sales_details = silver_sales_details.select(\n",
    "    *[trim(col(c)).alias(c) if dtype == \"string\" else col(c) for c, dtype in silver_sales_details.dtypes]\n",
    ")\n",
    "\n",
    "# display(silver_sales_details.select(\"sls_cust_id\",\"sls_sales\",\"sls_quantity\",\"sls_price\")\n",
    "#         .filter((col(\"sls_sales\") <= 0) | (col(\"sls_price\") <= 0) \n",
    "#                 | (col(\"sls_sales\").isNull()) | (col(\"sls_price\").isNull())))\n",
    "\n",
    "# Reconstructing Sales Price\n",
    "silver_sales_details = silver_sales_details.withColumn(\n",
    "    \"sls_sales\",\n",
    "    when((col(\"sls_sales\") <= 0) | (col(\"sls_sales\").isNull()), \n",
    "            col(\"sls_price\").cast(\"double\") * col(\"sls_quantity\").cast(\"double\"))\n",
    "    .otherwise(col(\"sls_sales\")) \n",
    ")\n",
    "\n",
    "silver_sales_details = silver_sales_details.withColumn(\n",
    "    \"sls_price\",\n",
    "    when((col(\"sls_price\") == 0) | (col(\"sls_price\").isNull()), \n",
    "         when((silver_sales_details[\"sls_quantity\"] != 0) | (silver_sales_details[\"sls_quantity\"].isNull()),\n",
    "            silver_sales_details[\"sls_sales\"] / silver_sales_details[\"sls_quantity\"])\n",
    "        .otherwise(0))\n",
    "   .when((col(\"sls_price\") < 0), \n",
    "         abs(col(\"sls_price\")))\n",
    "   .otherwise(col(\"sls_price\"))\n",
    ")\n",
    "\n",
    "# display(silver_sales_details.select(\"sls_cust_id\",\"sls_sales\",\"sls_quantity\",\"sls_price\")\n",
    "#         .filter((col(\"sls_sales\") <= 0) | (col(\"sls_price\") <= 0) \n",
    "#                 | (col(\"sls_sales\").isNull()) | (col(\"sls_price\").isNull())))\n",
    "\n",
    "# Handling null sls_price values\n",
    "# silver_sales_details = silver_sales_details.fillna({'sls_price': 0})\n",
    "# display(\n",
    "#     silver_sales_details.filter(\n",
    "#         (\n",
    "#             (col(\"sls_order_dt\") > col(\"sls_ship_dt\")) & (col(\"sls_order_dt\") > col(\"sls_due_dt\"))\n",
    "#         )\n",
    "#     )\n",
    "# )\n",
    "\n",
    "write_to_one_csv(silver_sales_details, \"/Volumes/workspace/default/silver/source_crm\", \"sales_details.csv\")\n",
    "display(silver_sales_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0f56940-e0ed-4736-93c2-90278759a504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Source ERP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3f25638-fff4-436c-8df4-e6625b4717a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CUST_AZ12\n",
    "silver_CUST_AZ12 = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/Volumes/workspace/default/bronze/source_erp/CUST_AZ12.csv\")\n",
    "\n",
    "# Data Cleaning\n",
    "# Split CID to match cst_id\n",
    "silver_CUST_AZ12 = silver_CUST_AZ12.withColumn(\n",
    "    \"CID\",\n",
    "    when(col(\"CID\").contains(\"NAS\"), regexp_replace(col(\"CID\"), \"NAS\", \"\")).otherwise(col(\"CID\"))\n",
    ")\n",
    "\n",
    "from pyspark.sql.functions import col, sum as Fsum, current_date\n",
    "# # Count nulls per column\n",
    "# display(silver_CUST_AZ12.select([\n",
    "#     Fsum(col(c).isNull().cast(\"int\")).alias(f\"{c}_nulls\")\n",
    "#     for c in silver_CUST_AZ12.columns\n",
    "# ]))\n",
    "\n",
    "# Normalizing Gen\n",
    "silver_CUST_AZ12 = silver_CUST_AZ12.withColumn(\n",
    "    \"GEN\",\n",
    "    when(col(\"GEN\") == \"M\", \"Male\")\n",
    "   .when(col(\"GEN\") == \"F\", \"Female\")\n",
    "   .when(col(\"GEN\").isNull(), \"NA\")\n",
    "   .otherwise(col(\"GEN\"))    \n",
    ")\n",
    "         \n",
    "# display(silver_CUST_AZ12.select(\"GEN\").distinct())\n",
    "\n",
    "# Removing old customers < 1924-01-01\n",
    "silver_CUST_AZ12 = silver_CUST_AZ12.filter((col(\"BDATE\") > \"1924-01-01\") | (col(\"BDATE\") < current_date()))\n",
    "\n",
    "\n",
    "# Verify every id matches into both tables\n",
    "# display(silver_CUST_AZ12.join(silver_cust_info, silver_CUST_AZ12[\"CID\"] == silver_cust_info[\"cst_key\"], \"left_anti\"))\n",
    "\n",
    "write_to_one_csv(silver_CUST_AZ12, \"/Volumes/workspace/default/silver/source_erp/\", \"CUST_AZ12.csv\")\n",
    "display(silver_CUST_AZ12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51a8a05c-2756-40a8-b9ce-20c835279883",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# LOC_A101\n",
    "silver_LOC_A101 = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/Volumes/workspace/default/bronze/source_erp/LOC_A101.csv\")\n",
    "\n",
    "# Data cleaning\n",
    "# CID Normailiztion\n",
    "silver_LOC_A101 = silver_LOC_A101.withColumn(\n",
    "    \"CID\",\n",
    "    regexp_replace(col(\"CID\"), \"-\",\"\")\n",
    ")\n",
    "\n",
    "# CNTRY Normalization\n",
    "silver_LOC_A101 = silver_LOC_A101.withColumn(\n",
    "    \"CNTRY\",\n",
    "    when((col(\"CNTRY\") == \"USA\") | (col(\"CNTRY\") == \"US\"), \"United States\")\n",
    "   .when(col(\"CNTRY\") == \"DE\", \"Germany\")\n",
    "   .when(col(\"CNTRY\").isNull(), \"NA\")\n",
    "   .otherwise(col(\"CNTRY\"))\n",
    ")\n",
    "\n",
    "write_to_one_csv(silver_LOC_A101, \"/Volumes/workspace/default/silver/source_erp/\", \"LOC_A101.csv\")\n",
    "display(silver_LOC_A101.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b2b211c-aa8a-498e-b13e-4dfc18790dc9",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1752371346556}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PX_CAT_G1V2\n",
    "silver_PX_CAT_G1V2 = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/Volumes/workspace/default/bronze/source_erp/PX_CAT_G1V2.csv\")\n",
    "\n",
    "write_to_one_csv(silver_PX_CAT_G1V2, \"/Volumes/workspace/default/silver/source_erp/\", \"PX_CAT_G1V.csv\")\n",
    "display(silver_PX_CAT_G1V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "811abd5a-f114-4962-9fa4-786a642881c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ccc6bc1-0ba4-4bea-b1c7-d47a836c070d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
